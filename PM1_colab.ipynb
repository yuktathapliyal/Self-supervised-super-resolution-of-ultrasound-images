{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PM1_github.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofQyrGfAw2Yg"
      },
      "source": [
        "\n",
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHxiPIQYw8V9"
      },
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import timeit\n",
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import skimage\n",
        "from skimage.util import random_noise\n",
        "import tensorflow as tf\n",
        "from time import strftime, localtime\n",
        "import random\n",
        "import pywt\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose,\\\n",
        "                                    GlobalAveragePooling2D, AveragePooling2D, MaxPool2D, UpSampling2D,\\\n",
        "                                    BatchNormalization, Activation, ReLU, Flatten, Dense, Input,\\\n",
        "                                    Add, Multiply, Concatenate, Softmax\n",
        "from tensorflow.keras import initializers, regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.activations import softmax\n",
        "from keras.applications.vgg19 import VGG19\n",
        "tf.keras.backend.set_image_data_format('channels_last')\n",
        "import keras.backend as K\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShKyydmN8e1t"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iADAE4Du8lg4"
      },
      "source": [
        "class DWT_downsampling(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Chintan, (2021) Image Denoising using Deep Learning [Github]. \n",
        "    https://github.com/chintan1995/Image-Denoising-using-Deep-Learning/blob/main/Models/MWCNN_256x256.ipynb\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "    def call(self, x):\n",
        "  \n",
        "        x1 = x[:, 0::2, 0::2, :] #x(2i−1, 2j−1)\n",
        "        x2 = x[:, 1::2, 0::2, :] #x(2i, 2j-1)\n",
        "        x3 = x[:, 0::2, 1::2, :] #x(2i−1, 2j)\n",
        "        x4 = x[:, 1::2, 1::2, :] #x(2i, 2j)   \n",
        "\n",
        "        x_LL = x1 + x2 + x3 + x4\n",
        "        x_LH = -x1 - x3 + x2 + x4\n",
        "        x_HL = -x1 + x3 - x2 + x4\n",
        "        x_HH = x1 - x3 - x2 + x4\n",
        "\n",
        "        return Concatenate(axis=-1)([x_LL, x_LH, x_HL, x_HH])\n",
        "\n",
        "class IWT_upsampling(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Chintan, (2021) Image Denoising using Deep Learning [Github]. \n",
        "    https://github.com/chintan1995/Image-Denoising-using-Deep-Learning/blob/main/Models/MWCNN_256x256.ipynb\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "    def call(self, x):\n",
        "        \n",
        "        x_LL = x[:, :, :, 0:x.shape[3]//4]\n",
        "        x_LH = x[:, :, :, x.shape[3]//4:x.shape[3]//4*2]\n",
        "        x_HL = x[:, :, :, x.shape[3]//4*2:x.shape[3]//4*3]\n",
        "        x_HH = x[:, :, :, x.shape[3]//4*3:]\n",
        "\n",
        "        x1 = (x_LL - x_LH - x_HL + x_HH)/4\n",
        "        x2 = (x_LL - x_LH + x_HL - x_HH)/4\n",
        "        x3 = (x_LL + x_LH - x_HL - x_HH)/4\n",
        "        x4 = (x_LL + x_LH + x_HL + x_HH)/4 \n",
        "\n",
        "        y1 = K.stack([x1,x3], axis=2)\n",
        "        y2 = K.stack([x2,x4], axis=2)\n",
        "        shape = K.shape(x)\n",
        "        return K.reshape(K.concatenate([y1,y2], axis=-1), K.stack([shape[0], shape[1]*2, shape[2]*2, shape[3]//4]))\n",
        "\n",
        "class Conv_block(tf.keras.layers.Layer):\n",
        "    def  __init__(self, num_filters=64, kernel_size=3, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_filters=num_filters\n",
        "        self.kernel_size=kernel_size\n",
        "        self.initializer = tf.keras.initializers.Orthogonal()\n",
        "        self.conv_1 = Conv2D(filters=self.num_filters, kernel_size=self.kernel_size, padding='same',kernel_initializer=self.initializer)\n",
        "        self.conv_2 = Conv2D(filters=self.num_filters, kernel_size=self.kernel_size, padding='same',kernel_initializer=self.initializer)\n",
        "        self.conv_3 = Conv2D(filters=self.num_filters, kernel_size=self.kernel_size, padding='same',kernel_initializer=self.initializer)\n",
        "        self.conv_4 = Conv2D(filters=self.num_filters, kernel_size=self.kernel_size, padding='same',kernel_initializer=self.initializer)\n",
        "    \n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'num_filters': self.num_filters,\n",
        "            'kernel_size':self.kernel_size\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, X):\n",
        "        X = self.conv_1(X)\n",
        "        X = ReLU()(X)\n",
        "        X = self.conv_2(X)\n",
        "        X = ReLU()(X)\n",
        "        X = self.conv_3(X)\n",
        "        X = ReLU()(X)\n",
        "        X = self.conv_4(X)\n",
        "        X = ReLU()(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "def build_model2():\n",
        "  input = Input(shape=(None, None, 3))                              \n",
        "\n",
        "  cb_1 = Conv_block(num_filters = 64)(input)                        \n",
        "\n",
        "  dwt = DWT_downsampling()(cb_1)                                    \n",
        "\n",
        "  cb_2 = Conv_block(num_filters=64)(dwt)                            \n",
        "\n",
        "  c_1 = Conv2D(filters = 256, kernel_size=3, strides=1, padding='same', activation='relu',kernel_initializer=tf.keras.initializers.Orthogonal())(cb_2)\n",
        "                                                                    \n",
        "  iwt = IWT_upsampling()(c_1)                                      \n",
        "\n",
        "  cb_3 = Conv_block(num_filters=64)(Add()([iwt, cb_1]))             \n",
        "\n",
        "  c_2 = Conv2D(filters = 3, kernel_size=3, strides=1, padding='same', activation='linear',kernel_initializer=tf.keras.initializers.Orthogonal())(cb_3)\n",
        "                                                                    \n",
        "  output = tf.keras.layers.Add()([c_2, input])\n",
        "\n",
        "  return Model(inputs = input, outputs = output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruGn7jaCbCH-"
      },
      "source": [
        "# Data pre-process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_aP6-_DbGvh"
      },
      "source": [
        "def load_img(file_name):\n",
        "    \n",
        "    image = cv2.imread(file_name, cv2.IMREAD_UNCHANGED)               \n",
        "                                                                      \n",
        "    if len(image.shape) == 3:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)                \n",
        "    else:\n",
        "        image = np.stack((image,) * 3, axis=-1)                       \n",
        "    if image.shape[0] == 1 or image.shape[0] == 3:\n",
        "        image = np.moveaxis(image, 0, -1)\n",
        "    image = image.astype('float32')                                   \n",
        "    \n",
        "    return image\n",
        "\n",
        "def add_noise(image, sigma):\n",
        "    row, col, ch = image.shape\n",
        "\n",
        "    noise = np.random.normal(0, sigma, (row, col, ch))    \n",
        "    \n",
        "    noise = noise.astype('float32')                                 \n",
        "    \n",
        "    noisy = np.clip((image + noise), 0, 255)                          \n",
        "                                                                     \n",
        "    return noisy\n",
        "\n",
        "def augment_parent(image,\n",
        "                   shear_scale_prob,\n",
        "                   leave_as_is_probability,\n",
        "                   crop_size):\n",
        "  \n",
        "  random_prob = np.random.rand()\n",
        "  random_augment = np.random.rand()\n",
        "\n",
        "  if random_prob < leave_as_is_probability:\n",
        "    mode = 'leave_as_is'\n",
        "  else:\n",
        "    mode = 'random_augment'\n",
        "  \n",
        "  image_h, image_w , _ = image.shape\n",
        "\n",
        "  if mode == 'leave_as_is':\n",
        "    hr_parent = image\n",
        "  else: \n",
        "    scaled_parent_h = image.shape[0]\n",
        "    scaled_parent_w = image.shape[1]\n",
        "    if random_augment > shear_scale_prob:\n",
        "      shear_x = np.random.randn()*0.25\n",
        "      shear_y = np.random.randn()*0.25\n",
        "      scale_x = np.random.randn()*0.15\n",
        "      scale_y = np.random.randn()*0.15\n",
        "      transform_matrix = np.array([[1+scale_x, shear_x, 0],[shear_y, 1+scale_y, 0]])\n",
        "      hr_parent = cv2.warpAffine(image, transform_matrix, (scaled_parent_w,scaled_parent_h))\n",
        "    if random_augment > 1:\n",
        "      if scaled_parent_h > crop_size:\n",
        "        start_h = int((scaled_parent_h - crop_size)/2)\n",
        "        end_h = int(start_h + crop_size)\n",
        "        hr_parent = image[start_h : end_h, :,:]\n",
        "      if scaled_parent_w > crop_size:\n",
        "        start_w = int((scaled_parent_w - crop_size)/2)\n",
        "        end_w = int(start_w + crop_size)\n",
        "        hr_parent = image[:, start_w : end_w, :]\n",
        "    else:\n",
        "      while (scaled_parent_h-1 < crop_size) or (scaled_parent_w-1 < crop_size):\n",
        "        crop_size -= 4\n",
        "      w_crop_diff = scaled_parent_w - crop_size\n",
        "      h_crop_diff = scaled_parent_h - crop_size\n",
        "      top_left_x_coordinate = np.random.randint(0, w_crop_diff)\n",
        "      top_left_y_coordinate = np.random.randint(0, h_crop_diff)\n",
        "      hr_parent = image[top_left_y_coordinate:top_left_y_coordinate+crop_size, top_left_x_coordinate:top_left_x_coordinate+crop_size,:]\n",
        "\n",
        "    random_rot = random.randint(0,7)\n",
        "    hr_parent = np.rot90(hr_parent, random_rot, axes = (0,1))\n",
        "    if random_rot > 3 :\n",
        "        hr_parent = np.fliplr(hr_parent)\n",
        "\n",
        "  if hr_parent.shape[0]%2 != 0:\n",
        "      hr_parent = hr_parent[:-1,:,:]\n",
        "  if hr_parent.shape[1]%2 !=0:\n",
        "      hr_parent = hr_parent[:,:-1,:]\n",
        "        \n",
        "  return hr_parent\n",
        "\n",
        "def parent_to_child(hr_parent, scale_factor, sigma):\n",
        "\n",
        "  scale_down = 1/scale_factor\n",
        "  random_chooser = np.random.rand()\n",
        "  downsample_prob = 0.5\n",
        "  if random_chooser < downsample_prob:\n",
        "    lr_child = hr_parent\n",
        "  else:\n",
        "    lr_child = cv2.resize(hr_parent, None, fx = scale_down, fy = scale_down, interpolation = cv2.INTER_CUBIC)\n",
        "  lr_child = add_noise(lr_child, sigma)\n",
        "\n",
        "  return lr_child\n",
        "\n",
        "def hr_lr_generator(image, \n",
        "                    scale_factor, \n",
        "                    shear_scale_prob,\n",
        "                    leave_as_is_probability,\n",
        "                    crop_size,\n",
        "                    sigma):\n",
        "\n",
        "  while True:\n",
        "    hr_parent = augment_parent(image,\n",
        "                               shear_scale_prob,\n",
        "                               leave_as_is_probability,\n",
        "                               crop_size)\n",
        "    \n",
        "    lr_child = parent_to_child(hr_parent, scale_factor, sigma)\n",
        "    if lr_child.shape != hr_parent.shape:\n",
        "      lr_child = cv2.resize(lr_child, (hr_parent.shape[1], hr_parent.shape[0]), interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "    y = np.expand_dims(hr_parent, axis = 0)\n",
        "    x = np.expand_dims(lr_child, axis=0)\n",
        "    \n",
        "    yield x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozxDYI2n3Qc3"
      },
      "source": [
        "# Other processing steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk7uix0J4RPn"
      },
      "source": [
        "def predict_image(image, scale_factor):\n",
        "\n",
        "  \"\"\"\n",
        "  This function predicts the original image on the trained model.\n",
        "  It takes the original image and interpolates it by scale_factor, expands image dimesnions to 4d, takes prediction,\n",
        "  and outputs 8 bit image.\n",
        "  \"\"\"\n",
        "\n",
        "  image_upscaled = cv2.resize(image, None, fx = scale_factor, fy = scale_factor, interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "  image_upscaled = np.expand_dims(image_upscaled, axis = 0)\n",
        "\n",
        "  super_image = model.predict(image_upscaled)\n",
        "\n",
        "  super_image = np.squeeze(super_image, axis=0)\n",
        "\n",
        "  super_image = cv2.convertScaleAbs(super_image)\n",
        "\n",
        "  return super_image\n",
        "\n",
        "\n",
        "\"\"\"# Gradual SR\"\"\"\n",
        "\n",
        "def get_gradual_factors(SR_factor, gradual_increase_value):\n",
        "  gradual_SR_list = [SR_factor]\n",
        "\n",
        "  sr_fact = SR_factor/gradual_increase_value\n",
        "\n",
        "  while (sr_fact) != 1:\n",
        "    gradual_SR_list.append(int(sr_fact))\n",
        "    sr_fact = sr_fact/gradual_increase_value\n",
        "\n",
        "  \n",
        "  gradual_SR_list.reverse()\n",
        "  return gradual_SR_list\n",
        "\n",
        "def get_images_paths(input_pd):\n",
        "  root = pathlib.Path(input_pd)\n",
        "  img_paths = list(sorted(root.rglob('*.png')))\n",
        "  img_paths_list = [str(path) for path in img_paths]\n",
        "  \n",
        "  return img_paths_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSCim8b74IMT"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PQrPpX5AF0V"
      },
      "source": [
        "input_pd = r''\n",
        "output_pd = r''\n",
        "\n",
        "drop_lr = 0.5 #\tfactor by which the learning rate will be reduced. new_lr = lr * factor\n",
        "num_epochs = 1500 # Number of epochs to run the model per image\n",
        "scale_factor= 2 # This is the Super-resolution factor\n",
        "sigma = 30 # Standard deviation (spread or “width”) of the normal distribution\n",
        "gradual_increase_value = 2 # Value with which the images are gradually super-resolved. This gradual increase factor is inspired by Shocher, Assaf & Cohen, Nadav & Irani, Michal. (2018). Zero-Shot Super-Resolution Using Deep Internal Learning. 3118-3126. 10.1109/CVPR.2018.00329. \n",
        "leave_as_is_probability = 0.2 # This is the probability associated with augmentation of hr parent. A higher leave_as_is_probability reduces probability of random augmentation in hr parent.\n",
        "shear_scale_prob = 0 # This the prabability associated with random shearing & scaling of HR parent during augmentations. A lower shear_scale_prob value prompts the model to increase the probability of random shearing & scaling, and vice-versa\n",
        "crop_size = 96 # This is the crop size of \n",
        "SR_factor = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g7yLEpvGpZg"
      },
      "source": [
        "callback_list = [tf.keras.callbacks.ReduceLROnPlateau(monitor = 'loss',\n",
        "                                                      factor = drop_lr,\n",
        "                                                      patience = 100,\n",
        "                                                      verbose = 1,\n",
        "                                                      mode = 'min',\n",
        "                                                      min_delta = 0.001,\n",
        "                                                      cooldown = 20,\n",
        "                                                      min_lr = 0.00000001),\n",
        "                  tf.keras.callbacks.EarlyStopping(monitor = 'loss',\n",
        "                                                   min_delta = 0.0001,\n",
        "                                                   patience = 350,\n",
        "                                                   verbose = 1,\n",
        "                                                   mode = 'min')\n",
        "]\n",
        "\n",
        "model = build_model2()\n",
        "\n",
        "mae_loss_object = tf.keras.losses.MeanAbsoluteError()\n",
        "vgg19 = VGG19(include_top=False, weights='imagenet')\n",
        "vgg19.trainable = False\n",
        "for l in vgg19.layers:\n",
        "    l.trainable = False\n",
        "vgg_model = Model(inputs=vgg19.input, outputs=vgg19.get_layer('block1_conv2').output)\n",
        "vgg_model.trainable = False\n",
        "\n",
        "def custom_loss(y_true, y_pred):    \n",
        "    mae_loss = mae_loss_object(y_true, y_pred)\n",
        "    vgg_loss = K.mean(K.square(vgg_model(y_true) - vgg_model(y_pred)))\n",
        "    vgg_loss_adjusted = (1 - (1/(1+vgg_loss)))*10\n",
        "    \n",
        "    return mae_loss + vgg_loss_adjusted\n",
        "\n",
        "\n",
        "\n",
        "gradual_SR_list = get_gradual_factors(SR_factor, gradual_increase_value)\n",
        "\n",
        "print('Scaling gradually in order:', gradual_SR_list)\n",
        "\n",
        "scale_fact = gradual_increase_value\n",
        "\n",
        "date_time = strftime('_%b_%d_%H_%M_%S', localtime())\n",
        "super_dir = output_pd + '/' + date_time + '/' + '_super'\n",
        "#avg_dir = output_pd + '/' + date_time + '/' + '_avg'\n",
        "#median_dir = output_pd + '/' + date_time + '/' + '_median'\n",
        "\n",
        "os.makedirs(super_dir)\n",
        "#os.makedirs(avg_dir)\n",
        "#os.makedirs(median_dir)\n",
        "start = timeit.default_timer()\n",
        "for file in os.listdir(input_pd):\n",
        "  model.compile(loss = custom_loss, optimizer = Adam(learning_rate=0.001))\n",
        "  model.summary()\n",
        "\n",
        "  image_path = os.path.join(input_pd, '%s' %file)\n",
        "  image = load_img(image_path)\n",
        "  print('starting training for', file)\n",
        "\n",
        "  for i in range(len(gradual_SR_list)):\n",
        "    \n",
        "    model.fit(hr_lr_generator(image, \n",
        "                              scale_factor=scale_fact, \n",
        "                              shear_scale_prob = shear_scale_prob,\n",
        "                              leave_as_is_probability = leave_as_is_probability,\n",
        "                              crop_size = crop_size,\n",
        "                              sigma = sigma),\n",
        "              batch_size = 1,\n",
        "              epochs = num_epochs,\n",
        "              verbose =1, \n",
        "              callbacks = callback_list, \n",
        "              steps_per_epoch = 1)\n",
        "    \n",
        "    super_image = predict_image(image, scale_factor = scale_fact)\n",
        "   \n",
        "    image = super_image \n",
        "    # avg, median = accumulated_result(image, scale_factor = scale_fact)\n",
        "\n",
        "  plt.imsave(os.path.join(super_dir,'%s' %file), super_image, format = 'png')\n",
        "  #plt.imsave(os.path.join(avg_dir,'%s' %file), avg, format = 'png')\n",
        "  #plt.imsave(os.path.join(median_dir,'%s' %file), median, format = 'png')\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "  \n",
        "stop = timeit.default_timer()\n",
        "print('Done!!!')\n",
        "print('Time take for all images is', stop-start, 'seconds')\n",
        "input_pd, output_pd = None, None\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
